{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction to Web Scraping**\n",
        "\n",
        "**What is Web Scraping?**\n",
        "\n",
        "\n",
        "*   Web scraping is the process of gathering information from the Internet.\n",
        "*   Web\n",
        "scraping is very imperative technique which is used to generate structured data on the basis of available unstructured data on the web.\n",
        "* Scaping generated structured data then stored in central database and analyze in spreadsheets.\n",
        "* Traditional copy-and-paste, Text grapping and regular expression matching, HTTP programming, HTML parsing, DOM parsing, Webscraping software, Vertical aggregation platforms, Semantic annotation recognizing and Computer vision web-page analyzers are some of the common techniques used for data scraping.\n",
        "\n"
      ],
      "metadata": {
        "id": "FsTEAzODQIth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Need for Web Scraping?** [Use Cases](https://www.webharvy.com/articles/web-scraper-use-cases.html)\n",
        "\n",
        "> The uses of Web Scraping for business as well as personal requirements are endless. Each business or individual has their own specific need for gathering data.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://www.webharvy.com/images/web%20scraping%20uses.png\">\n",
        "</p>\n",
        "\n",
        "1.   Monitoring of Price\n",
        "2.   Possible Market Trends\n",
        "3.   Keeping A Watch on Your Competitors\n",
        "4.   Maintaining Your Brand Identity\n",
        "5.   Social Media Management\n",
        "6.   SEO Enhancement\n",
        "7.   Knowing Your Targeted Audience\n",
        "8.   Improvising Better Solutions\n",
        "9.   Targeted Ads\n",
        "10.  Tracking Trends\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5cL-C74FRj-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outline for Scraping**\n",
        "1. Design considerations\n",
        "2. Crawling\n",
        "3. Scraping\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"http://raghudathesh.weebly.com/uploads/4/8/9/6/48968251/scraping-considatrations_orig.png\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "nDP9XZZQvazF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **For design considerations**\n",
        "1. What should be the output?\n",
        "> * Type of information\n",
        "> * Quality requirements\n",
        "2. What is the best suited input?\n",
        "3. Which method to get from input to output?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zh-PorIrv3-X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK0oEjKN-KVs"
      },
      "source": [
        "# **The essential fundamentals of web scraping are:**\n",
        "\n",
        "\n",
        "*   To understand the basics of HTML and CSS.\n",
        "*   HTML is used to give structure for a web page and CSS beautify the webpage.\n",
        "*   To explore the web page structure and usage of developer tools.\n",
        "*   To make HTTP requests and get HTML responses.\n",
        "*   To get specific structured information using beautifulsoup.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jfu3HK075WY"
      },
      "source": [
        "# **BeautifulSoup** [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "\n",
        "\n",
        "*   Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages.\n",
        "*   Say you’ve found some webpages that display data relevant to your work/research, such as date or address information, but that do not provide any way of downloading the data directly. Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information.\n",
        "*   It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web.\n",
        "*   This process is suitable for static content which is available by making an HTTP request to get the webpage content\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX8oxKST-FLg"
      },
      "source": [
        "# **Basic Termes in Web Scraping**\n",
        "\n",
        "1.   **Crawler**: is a web bot that visits a stack of web pages and accumulates the links (URLs) of the nodes, deriving new URLs from each new web page [html] that it visits. Crawler might or might not get pages’ info in a data storage. It does not go deep unless programmed explicitly.\n",
        "2.   **Scraper**: is a bot that visits web pages of a given set of URLs. It does not collect new URLs (as a crawler does). It rather visits pre-collected URLs and retrieves relevant data to store into a data storage.\n",
        "3.   **Parser**: is an [offline] robot that processes or analyses given data to dervie a proper data structures. It retrieves information from [unstructured] data, whether from data storage or directly from the web (e.g. HTML).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU-kRXbO_iFn"
      },
      "source": [
        "# **Types of Parser**\n",
        "\n",
        "1.   **html.parser** :  built-in, no extra dependencies needed.\n",
        "2.   **html5lib** : the most lenient (not strictly matches your pattern), better use it if HTML is broken.\n",
        "3.   **lxml** : the fastest.\n",
        "html2text check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItZUgMaFaF83"
      },
      "source": [
        "**How to Make a Soup out of HTML File**\n",
        "\n",
        "(Note: Here Soup mean way we prase the HTML Tree)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4W3L5Q_XnG2"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('intro_to_soup_html.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "# Make soup\n",
        "#Syntax = BeautifulSoup(html_data,parser)\n",
        "# Our parser is lxml or html.parser which we have installed\n",
        "\n",
        "html_file = read_file()\n",
        "print(html_file)\n",
        "print(\"------------------------------------\\n\")\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(html_file,'lxml')\n",
        "print(soup)\n",
        "print(\"------------------------------------\\n\")\n",
        "type(soup)\n",
        "\n",
        "# soup prettify\n",
        "print(soup.prettify())\n",
        "print(\"------------------------------------\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_qTOqPCaUA7"
      },
      "source": [
        "# **How to Make a Soup out of any Website HTML**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW_qGTKpbJ0g",
        "outputId": "4a5f4d96-aa7c-4b9b-a70d-c2bffe858dee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install fake_useragent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.2.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **fake_useragent:**[Library link](https://pypi.org/project/fake_user_agent/)\n",
        "Randomly generates a useragent for fetching a web page without a browser.\n",
        "\n"
      ],
      "metadata": {
        "id": "oGpdkFIPWMw5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqmM7H1lbFRe"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "ua = UserAgent()\n",
        "header = {'user-agent':ua.chrome}\n",
        "google_page = requests.get('https://www.google.com',headers=header)\n",
        "#print(google_page.content)\n",
        "\n",
        "soup = BeautifulSoup(google_page.content,'lxml') # html.parser\n",
        "\n",
        "print(soup.prettify())\n",
        "\n",
        "\n",
        "#identify some tags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requests: Issues an HTTP GET request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object."
      ],
      "metadata": {
        "id": "r7VfQGnnZNZr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-AS6aMRdNoe"
      },
      "source": [
        "# Analysis to HTML **Tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol4oKTohdSPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fb258f-e08c-465f-a329-07a505812f1a"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('tags.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "#print(soup)\n",
        "print(\"------------------------------------\\n\")\n",
        "# Accessing tags\n",
        "meta = soup.meta\n",
        "#print(meta) # gives us the first occurance of meta tag.\n",
        "print(\"------------------------------------\\n\")\n",
        "#div = soup.div\n",
        "print(div) # gives us the first occurance of tag ->div.\n",
        "\n",
        "# tag methods\n",
        "'''\n",
        "name\n",
        "-- attributes\n",
        ".get() method\n",
        "dictionary\n",
        "'''\n",
        "#print(\"Value of Charset via get method is: \")\n",
        "#print(meta.get(\"charset\"))\n",
        "\n",
        "#print(\"Value of Charset via  dictonary is: \")\n",
        "#print(meta[\"charset\"]) # can be treated as dictionary\n",
        "\n",
        "# modify attributes at runtime\n",
        "body = soup.body\n",
        "#print(body) # prints entire body content\n",
        "#print(body['style'])  # output will be blank as there is no style\n",
        "body['style'] = 'some style'\n",
        "#print(body['style']) # returns some style\n",
        "\n",
        "'''\n",
        " Multi valued attributes\n",
        "'''\n",
        "print(body['class']) # here class has two attributes(list): first and second"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "<div>\n",
            "<p style=\"color:red;\">In first div</p>\n",
            "</div>\n",
            "['first', 'second']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6SlajmAihRQ"
      },
      "source": [
        "# **Navigable Strings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAjLpjHikkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b3b406-b916-4a5c-f4c1-732e2a2b5311"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('intro_to_soup_html.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# Navigable strings in the HTML file are: Intro_to_soup, In first div, In second div\n",
        "\n",
        "# To access string inside a tag use .string method (Accessing Navigable strings )\n",
        "title = soup.title\n",
        "\n",
        "#print(title)         #Complete HTML Element is printed\n",
        "#print(title.string)  #String in the HTML element is printed\n",
        "\n",
        "\n",
        "# .replace_with(\"\") function            -- navigable string\n",
        "print(\"Before replacing:\")\n",
        "print(title)\n",
        "\n",
        "title.string.replace_with(\"title has been changed\")# replaces \"Intro_to_soup\" to \"title has been changed\"\n",
        "\n",
        "print(\"After replacing:\")\n",
        "print(title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before replacing:\n",
            "<title>Intro_to_soup</title>\n",
            "After replacing:\n",
            "<title>title has been changed</title>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gLMNWn_9o6C"
      },
      "source": [
        "#**Navigating Through tag Names**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfTMbJx_9x_O"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# example  -- accessing tags directely from their tag names\n",
        "title = soup.title\n",
        "print(title) # prints 1st title tag\n",
        "print(\"------------------------------------\\n\")\n",
        "p = soup.p\n",
        "print(p) # prints 1st p tag\n",
        "print(\"------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6JN6V_t_lZ1"
      },
      "source": [
        "# **Navigating Through Child tag**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYdEFNb_uaC"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# tag.contents         -- returns a list of children\n",
        "head = soup.head\n",
        "#print(head.contents)\n",
        "#print(\"------------------------------------\\n\")\n",
        "for child in head.contents:\n",
        "    #print(child if child is not None else'')\n",
        "    #print(\"------------------------------------\\n\")\n",
        "    pass\n",
        "\n",
        "body = soup.body\n",
        "#print(body.contents) # to illustrate new line character but from web it may not be their\n",
        "#print(\"------------------------------------\\n\")\n",
        "for child in body.contents:\n",
        "    #print(child if child is not None else '', end='\\n\\n\\n\\n')  #here end='\\n\\n\\n\\n' is written only to differntiating between tags works fine if deleted\n",
        "    pass\n",
        "\n",
        "#------ nop------------\n",
        "# .children         -- returns an iterator\n",
        "#for child in body.children:\n",
        "    #print(child if child is not None else '', end='\\n\\n\\n\\n')\n",
        "    #pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTt-opaHMhpH"
      },
      "source": [
        "# Navigating with Beautifulsoup - Going Down - use three_sisters.html\n",
        "\n",
        "There are 3 types of movement across html Parse tree\n",
        "\n",
        "1.   Down the Tree - body tag to P tag\n",
        "2.   Up the Tree - P tag to body tag\n",
        "3.   Sideways Movement - P tag to P tag Movement\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXxh9RU6_0Kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a8613d-ed4d-480d-c57d-e44aa3f7be68"
      },
      "source": [
        "#This script describes how to move up in an html parse tree from a child tag\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "title = soup.title\n",
        "\n",
        "parent = title.parent\n",
        "#print(parent)   #prints the complete parent tag HTML Element\n",
        "#print(parent.name)   # method.name ---> gives Parent tag's name\n",
        "\n",
        "\n",
        "# .parent\n",
        "p = soup.p\n",
        "#print(p)  #prints first occurance of p tag\n",
        "#print(p.parent) #prints complte body tag, since it is the parent of p tag\n",
        "#print(p.parent.name) # prints only the name of the parent\n",
        "\n",
        "'''\n",
        "note: all p tags are siblings in the html\n",
        "Tree starts from soup --> has its child as HTML --> HTML has childerns as head and body --> head and boby has childrens depending on the structure of web pag\n",
        "'''\n",
        "\n",
        "# html\n",
        "html = soup.html\n",
        "#print(type(html.parent))         #   bs4 (top level parent of every parse tags) ---> html ---- prints the parent of html\n",
        "\n",
        "\n",
        "# soup\n",
        "#print(soup.parent) # returns none as it is at top of the hirerchey"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bs4.BeautifulSoup'>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV9sS_XeWJhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7706388-8798-47e8-91ee-c44d2af430ac"
      },
      "source": [
        "'''This script describes .parent method, how access all the\n",
        "parents of a perticular tag'''\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "'''\n",
        " .parents              --- returns a list (generator)  of parents\n",
        " we shall use 'a' tag\n",
        " 'a' tag has parent as 'p' tag which has parent as 'body' tag and so on\n",
        "\n",
        "#moving up the tree: a --> p --> body --> html --> beautifulsoup\n",
        "\n",
        "'''\n",
        "\n",
        "link = soup.a\n",
        "#print(link) # prints first a tag\n",
        "#print(link.parents) # returns generator object parents at mem location\n",
        "#print(link.parent) # returns P tag structure\n",
        "#print(link.parent.name) # returns a tag's parent name only\n",
        "\n",
        "for parent in link.parents:\n",
        "    #print(parent.name) # p --> body --> html --> doc\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p\n",
            "body\n",
            "html\n",
            "[document]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JNKez3gZC5R"
      },
      "source": [
        "# Navigating with Beautifulsoup - Going Sideway (moving through siblings) - use three_sisters.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6XSouGbZI9y"
      },
      "source": [
        "#This script demonstrates moving from current tag to next sibling tag\n",
        "#Here we are moving side ways\n",
        "\n",
        "#observer that first b and p tags are siblings\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "body = soup.body\n",
        "p = soup.body.p\n",
        "#print(p) #print first p tag with class title\n",
        "\n",
        "# body - contents\n",
        "#print(body.contents)\n",
        "\n",
        "\n",
        "#.next_sibling\n",
        "#our task now is to move from p tag \"title\" to next p tag \"story\".\n",
        "#observe the output of print(body.contents) their is a new line character \"\\n\"\n",
        "#and then the p tag \"story\"\n",
        "\n",
        "\n",
        "print(p.next_sibling) # prints nothing as it is new line character\n",
        "#print(p.next_sibling.next_sibling) #prints p tag \"story\". Moving side ways\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x60u0OexcMBG"
      },
      "source": [
        "'''\n",
        "This script demonstrates moving from current tag to previous sibling tag\n",
        "Here we are moving side ways\n",
        "Here we are moving from body tag to head tag\n",
        "'''\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "body = soup.body\n",
        "\n",
        "# contents - html\n",
        "#print(soup.html.contents) #prints complete html\n",
        "\n",
        "#we shall move from body tag to head tag\n",
        "# .previous_sibling\n",
        "#print(body.previous_sibling) # prints nothing as it is new line character\n",
        "#print(body.previous_sibling.previous_sibling) #prints head tag, sibling of body tag, moving up or previous sibling\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkwfVFFOeOF7"
      },
      "source": [
        "'''\n",
        "This script demonstrates moving from current tag to next tag and previous sibling tag.\n",
        "Here we are moving side ways.\n",
        "Here we are moving from 'p' tag to next 'p' tag, also to previous 'b' tag siblings.\n",
        "'''\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "p = (soup.body.p)\n",
        "#print(p)   #prints first 'p' tag. <p class=\"title\">\n",
        "\n",
        "\n",
        "# .next_siblings (after b tag it has two siblings i.e, p, p tags)\n",
        "#Use inline if to escape the '\\n': (value if contiditon else '')\n",
        "\n",
        "for sibling in p.next_siblings:\n",
        "  #print(sibling.name if sibling != '\\n' else '') # note: here we are omitting new line character see tree\n",
        "  pass\n",
        "\n",
        "\n",
        "# .previous_siblings (before first 'p' tag there is only one 'b' tag) # note: here we are omitting new line character see tree\n",
        "\n",
        "for sibling in p.previous_siblings:\n",
        "  print(sibling if sibling  != '\\n' else '')\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Case [Beautiful Soup]([URL](https://realpython.com/beautiful-soup-web-scraper-python/)) Assignment"
      ],
      "metadata": {
        "id": "JfUq6EQxcc63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Regular Expressions"
      ],
      "metadata": {
        "id": "cygolLnj7U1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re   # regular expression module\n",
        "\n",
        "# specialised language which can be used to search for text within a given document with precision and efficiency\n",
        "\n",
        "# expression -- > compiled into bytecode    --> executed by a matching engine written in C\n",
        "\n",
        "# Usage in ourcase:\n",
        "# Matching Characters, Finding tags, Finding data through parse string.\n",
        "\n",
        "'''\n",
        "What is Regular Expression?\n",
        "A simple expression matches itself in the given string\n",
        "\n",
        "REGEX: abc\n",
        "\n",
        "String: abcdef\n",
        "\n",
        "Exception --> Metacharacters\n",
        "\n",
        "They don't match themselves\n",
        "\n",
        "Complete list of Metacharacters  -->         . ^ $ * + ? { } [ ] \\ | ( )\n",
        "\n",
        "'''\n",
        "\n",
        "# First Metacharacters which we will look at are --- >        [   ]\n",
        "\n",
        "'''\n",
        "# REGEX: b\n",
        "\n",
        "# String: [abcdef]  or [a-f] if we apply regex on this we get output as true, if any other character output will be FALSE\n",
        "\n",
        "# REGEX: 9\n",
        "\n",
        "\n",
        "# String: [12345] - [1-5]   : Output will be False.\n",
        "\n",
        "\n",
        "\n",
        "used for specifying a character class   - character class is a set of characters you wish to match\n",
        "\n",
        "for example if I've written the following regex :\n",
        "\n",
        "            [xyz]\n",
        "\n",
        "this will match any x,y or z character\n",
        "\n",
        "We could also give a range using hyphen,\n",
        "\n",
        "\n",
        "            [x-z]           --- equivalent to ---              [xyz]\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Z9Uxgc2E7Yzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compile function and Character class**"
      ],
      "metadata": {
        "id": "ecXIRCHt-hu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# re.compile(pattern)       -- returns a regex object\n",
        "\n",
        "#regex = re.compile('[ccca]')\n",
        "#regex = re.compile('[a-h]') #Range\n",
        "regex = re.compile('[a-zA-Z]') #Range\n",
        "\n",
        "\n",
        "# regex.match(string to match) -- returns None if no match else returns a match object\n",
        "print(regex.match('BA'))\n",
        "\n",
        "\n",
        "# character class\n",
        "\n",
        "# complement the set [^pattern]\n",
        "regex = re.compile('[^ccca]')\n",
        "#regex = re.compile('[a-h]') #Range\n",
        "#regex = re.compile('[a-zA-Z]') #Range\n",
        "#regex = re.compile('[+]')\n",
        "\n",
        "#print(regex.match('c'))\n",
        "\n",
        "\n",
        "\n",
        "# all metacharacters lose their meaning inside a character class\n",
        "#regex = re.compile('[+]')\n",
        "#print(regex.match('+'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czFPZvti-iTT",
        "outputId": "2d37fe2e-6f41-4798-aea2-9db903271b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='B'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Sequence"
      ],
      "metadata": {
        "id": "WXUuVGxKA-Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# special sequences\n",
        "\n",
        "\n",
        "# \\d        -- matches any decimal digit --     [0-9]\n",
        "\n",
        "regex = re.compile('\\d')\n",
        "\n",
        "\n",
        "# \\D        -- matches any non-digit character  -- [^0-9]\n",
        "\n",
        "regex = re.compile('\\D')\n",
        "\n",
        "# \\s        -- matches any whitespace character\n",
        "\n",
        "regex = re.compile('\\s')\n",
        "\n",
        "# \\S        -- matches any non-whitespace character\n",
        "\n",
        "regex = re.compile('\\S')\n",
        "\n",
        "# \\w        -- matches any alphanumeric character -- [a-zA-Z0-9_]\n",
        "\n",
        "regex = re.compile('\\w')\n",
        "\n",
        "# \\W        -- matches any non-alphanumeric character -- [^ a-zA-Z0-9_]\n",
        "\n",
        "regex = re.compile('\\W')"
      ],
      "metadata": {
        "id": "BKdkxy6pBAd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asterisk repeating things"
      ],
      "metadata": {
        "id": "p3qx8eN9Br_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# * character - this specifies that the previous character can be matched zero or more times, instead of exactly once.\n",
        "\n",
        "#Simple Regex\n",
        "#regex = re.compile('aaaaa') # to match 5 occurance of 'a'\n",
        "#for  * matching range is 0 to infinity\n",
        "regex = re.compile('a*')  # to match 500/50000 occurance of 'a'\n",
        "#print(regex.match('aaaaaaaccaa'))\n",
        "#print(regex.match('')) # -- lower limit is 0 and the upper limit is infinity\n",
        "\n",
        "regex = re.compile('[a-c]*')       # -- lower limit is 0 and the upper limit is infinity\n",
        "#print(regex.match('caaaaaaaaaaabcaaaaa'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG14H3XVBu_M",
        "outputId": "c706b792-ec74-412c-94b3-de2ba598f9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 0), match=''>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ++repeating+thing"
      ],
      "metadata": {
        "id": "Y-ZK6RxsGYxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# +  character -- this specifies that the previous character can be matched one or more times\n",
        "\n",
        "# difference from '*'-- 0 - infinity ,      for  '+'  matching range is 1 to infinity\n",
        "\n",
        "regex = re.compile('a+')\n",
        "#print(regex.match(''))\n",
        "#print(regex.match('a'))\n",
        "#print(regex.match('aaaaaaaaa'))\n",
        "\n",
        "\n",
        "# using character classes\n",
        "#regex = re.compile('[a-c]*')\n",
        "regex = re.compile('[a-c]+')\n",
        "#print(regex.match('abcabcabc'))\n",
        "#print(regex.match(''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ioNqeUGaVm",
        "outputId": "b782a591-3aee-4786-c8bf-583699cf500d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 9), match='abcabcabc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ? and {m,n} repeating thing"
      ],
      "metadata": {
        "id": "HKlY9b2LHni5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# ? question mark -- says the previous character can either come once or not at all\n",
        "\n",
        "regex = re.compile('a?b')           # min - 0       max - 1\n",
        "#print(regex.match('b'))\n",
        "#print(regex.match('a'))\n",
        "#print(regex.match('ab'))\n",
        "#print(regex.match('aab'))\n",
        "\n",
        "\n",
        "# {m,n}    m and n are integer values   -- This qualifier means there must be at least m repetitions, and at most n\n",
        "\n",
        "regex = re.compile('a{2,4}')            # aa aaa aaaa\n",
        "#print(regex.match('a'))\n",
        "#print(regex.match('aa'))\n",
        "#print(regex.match('aaa'))\n",
        "#print(regex.match('aaaa'))\n",
        "#print(regex.match('aaaaa'))\n",
        "\n",
        "\n",
        "\n",
        "# * {0,}\n",
        "\n",
        "regex = re.compile('a{0,}')    # zero to infinite\n",
        "#print(regex.match(''))\n",
        "#print(regex.match('a'))\n",
        "#print(regex.match('aa'))\n",
        "#print(regex.match('aaa'))\n",
        "#print(regex.match('aaaa'))\n",
        "#print(regex.match('aaaaa'))\n",
        "#print(regex.match('aaaaaaaa'))\n",
        "\n",
        "# + {1,}\n",
        "\n",
        "# Assignment: Write different scenario\n",
        "\n",
        "\n",
        "# ? {0,1}\n",
        "# Assignment: Write different scenario\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg6M1SUfHWEj",
        "outputId": "c2dab9ec-79fb-44d2-8701-ac6ea3dc06a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 2), match='ab'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metacharacters Conti..."
      ],
      "metadata": {
        "id": "TIYNvIkzgWQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# ^ character   -- says that the string should start with\n",
        "regex = re.compile('^abc')\n",
        "\n",
        "\n",
        "# | character -- is the or operator\n",
        "\n",
        "regex = re.compile('a|b')\n",
        "\n",
        "# $ character -- matches the end of line\n",
        "\n",
        "regex = re.compile('abc$')\n",
        "\n"
      ],
      "metadata": {
        "id": "0tXecXkYgcJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction  to searching with Beautiful Soup\n",
        "\n",
        "**Use Acse: Say your tree has thousands of tags. How are you going navigate through thousands of tags to get to the tags desire.**\n",
        "* Solution:\n",
        ">* So here we come to searching. Beautiful Soup provides us with very strong methods and very efficient methods which return us the tags we want.\n",
        ">* It searches the whole parse tree for the tags we want and it gives us back to those tags.\n",
        ">* The most popular method for searching are \"find\" and \"find_all\".\n"
      ],
      "metadata": {
        "id": "XLi7lXOtl76M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# most popular methods\n",
        "\n",
        "# find()\n",
        "# find_all()        -- to keep it simple for now, it takes the tag name as parameter\n",
        "\n",
        "\n",
        "# Kinds of filters which we can use to retrieve tags - filters sent as parameter to find/find_all methods\n",
        "\n",
        "# string\n",
        "#print(soup.find_all('b')) #give a list of tags\n",
        "#print(soup.find_all('a'))\n",
        "\n",
        "# regular expression\n",
        "\n",
        "# tag names start with b\n",
        "\n",
        "regex = re.compile('^b')\n",
        "\n",
        "for tag in soup.find_all(regex):\n",
        "    #print(tag.name)\n",
        "    pass\n",
        "\n",
        "\n",
        "# tag names contains t\n",
        "\n",
        "regex = re.compile('t')\n",
        "\n",
        "for tag in soup.find_all(regex):\n",
        "    #print(tag.name)\n",
        "    pass\n",
        "\n",
        "\n",
        "# list\n",
        "\n",
        "# all a and b tags\n",
        "\n",
        "for tag in soup.find_all(['a','b']):\n",
        "    #print(tag.name)\n",
        "    pass\n",
        "\n",
        "\n",
        "# function\n",
        "\n",
        "# just giving an example here - we'll discuss this more when we implement find_all\n",
        "\n",
        "def has_class(tag):\n",
        "    return tag.has_attr('class')\n",
        "\n",
        "for tag in soup.find_all(has_class):\n",
        "  #print(tag.name)\n",
        "  pass"
      ],
      "metadata": {
        "id": "usefj-GimCdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## find_all function"
      ],
      "metadata": {
        "id": "iz6R8ZNCq5V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# Signature: find_all(name, attrs, recursive, string, limit, **kwargs) #kwargs =keywork arguments\n",
        "\n",
        "# name parameter can take regex object or string or True or function\n",
        "\n",
        "a_tags = soup.find_all('a')\n",
        "#print(a_tags)\n",
        "\n",
        "# attrs parameter\n",
        "\n",
        "# dicitonary\n",
        "\n",
        "\n",
        "attr = {'class' : 'sister'}\n",
        "first_a = soup.find_all('a' , attrs=attr)\n",
        "#print(first_a)\n",
        "\n",
        "attr = {'class':'story'}\n",
        "first_a = soup.find_all(attrs=attr)\n",
        "#print(first_a)\n",
        "\n",
        "attr = {'class' : 'sister', 'id' : 'link1'}\n",
        "first_a = soup.find_all('a' , attrs=attr)\n",
        "#print(first_a)\n",
        "\n",
        "\n",
        "# limit parameter used to limit number of tags to return\n",
        "\n",
        "a_tags = soup.find_all('a',limit=2)\n",
        "#print(a_tags)"
      ],
      "metadata": {
        "id": "MWlNiyUSq9Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# Signature: find_all(name, attrs, recursive, string, limit, **kwargs)\n",
        "\n",
        "# string parameter: accepts srting or regex parameter\n",
        "regex = re.compile('Elsie')\n",
        "#regex = re.compile('story')\n",
        "\n",
        "#tag = soup.find_all(string=regex)\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "# **kwargs arguments\n",
        "tags = soup.find_all(class_='sister')\n",
        "#tags = soup.find_all(class_='story')\n",
        "for tag in tags:\n",
        "  #print(tag)\n",
        "  pass\n",
        "\n",
        "# to write the class attribute of a tag - use       class_          because simple class is a keyword in Python\n",
        "\n",
        "\n",
        "# recursive parameter\n",
        "\n",
        "\n",
        "title = soup.find_all('title',recursive=False) # output is nil as it find only html tag not its childern; try with True\n",
        "#print(title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHRTXCoTvKXt",
        "outputId": "697e63c0-efca-4b63-ed3c-5329b75a25cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Function"
      ],
      "metadata": {
        "id": "EysoEgeWxIU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "def read_file():\n",
        "    file = open('three_sisters.html')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "# Signature: find(name, attrs, recursive, string, **kwargs)     - limit is not present as in find all function\n",
        "\n",
        "# returns a single object if found      -- in case of multiple objects, it returns the first one it finds\n",
        "\n",
        "tag = soup.find('a')\n",
        "print(tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQDEtGXExLgg",
        "outputId": "6031c978-2a95-4a62-c6e7-1a9be485bb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
            "                Elsie\n",
            "            </a>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projects\n"
      ],
      "metadata": {
        "id": "uzJV7QfEzUzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "P1: # **P1 - Scraping Quotes from website**\n",
        "\n",
        "**URL:** [Quotes Website](http://www.values.com/inspirational-quotes)\n",
        "\n",
        "**scrapes the website and saves quotes to a file inspirational_quotes.csv**"
      ],
      "metadata": {
        "id": "H8Z0YJvP5ybU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Python program to scrape website and save quotes to a file inspirational_quotes.csv\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "#URL = \"http://www.values.com/inspirational-quotes\"\n",
        "URL = \"https://www.passiton.com/inspirational-quotes\"\n",
        "\n",
        "r = requests.get(URL)\n",
        "\n",
        "soup = BeautifulSoup(r.content, 'html5lib')\n",
        "\n",
        "quotes=[] # a list to store quotes\n",
        "\n",
        "table = soup.find('div', attrs = {'id':'all_quotes'})\n",
        "\n",
        "for row in table.findAll('div',\n",
        "\t\t\t\t\t\tattrs = {'class':'col-6 col-lg-4 text-center margin-30px-bottom sm-margin-30px-top'}):\n",
        "\tquote = {}\n",
        "\tquote['theme'] = row.h5.text\n",
        "\tquote['url'] = row.a['href']\n",
        "\tquote['img'] = row.img['src']\n",
        "\tquote['lines'] = row.img['alt'].split(\" #\")[0]\n",
        "\tquote['author'] = row.img['alt'].split(\" #\")[1]\n",
        "\tquotes.append(quote)\n",
        "\n",
        "filename = 'inspirational_quotes.csv'\n",
        "with open(filename, 'w', newline='') as f:\n",
        "\tw = csv.DictWriter(f,['theme','url','img','lines','author'])\n",
        "\tw.writeheader()\n",
        "\tfor quote in quotes:\n",
        "\t\tw.writerow(quote)"
      ],
      "metadata": {
        "id": "BJt9PQEZ59zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **P2 - Scraping Covid-19 stats**\n",
        "\n",
        "URL: [COVID-19 STATS COUNTRY WISE](https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/)"
      ],
      "metadata": {
        "id": "minzAvki6KfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URl to Scrap: https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import texttable as tt\n",
        "\n",
        "# URL for scrapping data\n",
        "url = 'https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/'\n",
        "\n",
        "# get URL's html\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "data = []\n",
        "\n",
        "# soup.find_all('td') will scrape every element in the url's table\n",
        "data_iterator = iter(soup.find_all('td'))\n",
        "# data_iterator is the iterator of the table\n",
        "\n",
        "# This loop will keep repeating till there is data available in the iterator\n",
        "while True:\n",
        "\ttry:\n",
        "\t\tcountry = next(data_iterator).text\n",
        "\t\tconfirmed = next(data_iterator).text\n",
        "\t\tdeaths = next(data_iterator).text\n",
        "\t\tcontinent = next(data_iterator).text\n",
        "\n",
        "\t\t# For 'confirmed' and 'deaths', make sure to remove the commas and convert to int\n",
        "\t\tdata.append((\n",
        "\t\t\tcountry,\n",
        "\t\t\tint(confirmed.replace(',', '')),\n",
        "\t\t\tint(deaths.replace(',', '')),\n",
        "\t\t\tcontinent\n",
        "\t\t))\n",
        "\n",
        "\t# StopIteration error is raised when there are no more elements left to iterate through\n",
        "\texcept StopIteration:\n",
        "\t\tbreak\n",
        "\n",
        "# Sort the data by the number of confirmed cases\n",
        "data.sort(key = lambda row: row[1], reverse = True)\n",
        "\n",
        "\n",
        "# create texttable object\n",
        "table = tt.Texttable()\n",
        "table.add_rows([(None, None, None, None)] + data)  # Add an empty row at the beginning for the headers\n",
        "table.set_cols_align(('c', 'c', 'c', 'c'))  # 'l' denotes left, 'c' denotes center, and 'r' denotes right\n",
        "table.header((' Country ', ' Number of cases ', ' Deaths ', ' Continent '))\n",
        "\n",
        "print(table.draw())\n"
      ],
      "metadata": {
        "id": "hjLccjyd6LRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **P3 - Scraping the data from wikipedia**\n",
        "\n",
        "URL: https://en.wikipedia.org/wiki/World_War_II"
      ],
      "metadata": {
        "id": "QLp2KsmO6R3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## scrap data from wikipedia\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "wiki=requests.get(\"https://en.wikipedia.org/wiki/World_War_II\")\n",
        "soup=BeautifulSoup(wiki.text,'html')\n",
        "print(soup.find('title'))\n",
        "\n",
        "\n",
        "# ### find html tags with classes\n",
        "\n",
        "ww2_contents=soup.find_all(\"div\",class_='toc')\n",
        "for i in ww2_contents:\n",
        "    print(i.text)\n",
        "\n",
        "\n",
        "overview=soup.find_all('table',class_='infobox vevent')\n",
        "for z in overview:\n",
        "    print(z.text)"
      ],
      "metadata": {
        "id": "5ugqEI3R6WVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}