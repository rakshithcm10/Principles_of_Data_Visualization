1. https://scrapy.org/

2. Documentation

3. Scrapy at glance

4. Installation Guide

5. To install Scrapy using conda, run in anaconda power shell in admin mode:

conda install -c conda-forge scrapy


6. Using a virtual environment (recommended) by I am doing in regular environment

7. Type scrapy in power shell
(base) PS C:\Windows\system32> scrapy
Scrapy 2.10.0 - no active project     ### initially we dont have any porject installed.

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

8. In Desktop create a folder I have named it as Scrapy_teaching_AIML_2023

9. In spyder set the path to above created location as: C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023

10. Go to project an create a project with name Scrapy_project1 and location C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023

11. In Power shell type the command: scrapy startproject quotetutorial

(base) PS C:\Windows\system32> cd C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023
(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023> scrapy startproject quotetutorial
New Scrapy project 'quotetutorial', using template directory 'C:\Users\MAHE\anaconda3\Lib\site-packages\scrapy\templates\project', created in:
    C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial

You can start your first spider with:
    cd quotetutorial
    scrapy genspider example example.com
(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023>

12. go to the quotetutorial path 

(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023>cd quotetutorial

(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial>

13. Again goto quotetutorial one level down
(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial>cd .\quotetutorial\

(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial\quotetutorial>

14. inspect the content of directory
(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial\quotetutorial>ls

    Directory: C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial\quotetutorial


Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
d-----        21-08-2023     10:32                spiders
-a----        21-08-2023     11:47            281 items.py
-a----        21-08-2023     11:47           3765 middlewares.py
-a----        21-08-2023     11:47            380 pipelines.py
-a----        21-08-2023     11:47           3454 settings.py
-a----        04-08-2023     22:12              0 __init__.py


15. Now in spyder change the project path to the above created spiders path 
    slect the file view go one step up you will be able to see the folder structure.


16. First file to discuss is settings.py

   Here BOT_NAME = "quotetutorial"

    # Obey robots.txt rules
    ROBOTSTXT_OBEY = True

go to browser and type https://www.amazon.in/robots.txt

   This file has imformation abouth which content to be scraped or not. say if the scraper is from amazon it may allow to scrape.

In setting.py: teach robort.txt, concurrent request, pipeline rest for assignment.

17. items.py

     used to store the entities/fields that are scraped from a website.
     from here we can convert string into int, float ect, also channalise data to data base or for further processing.
     These(items) are basically a container to hold the scraped data.
 
18. pipelines.py

Scraped data --> Item(container) --> Json, xml

Scraped data --> Item(container) --> Pipeline (connection est./ create table, invoke curser object etc)  --> data base sql,mango db

19. middleware.py

          Has many function definitions for performing different task like adding proxy, applying RE, formatting data etc.

20. Go to spyders folder and create your first spyder
click on spyder folder and create new file with name quotes_spyder.py

# -*- coding: utf-8 -*-
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    allowed_domains = ["quotes.toscrape.com"]
    start_urls = (
        'http://quotes.toscrape.com/',
    )

    def parse(self, response):
        #title = response.css('title').extract() #output=tage + string
	title = response.css('title::text').extract()  #output= string
        yield {'titletest': title}

21. In power shell navigate into the spyders folder 

(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial\quotetutorial>cd .\spiders\

(base) PS C:\Users\MAHE\Desktop\Scrapy_teaching_AIML_2023\quotetutorial\quotetutorial\spiders>scrapy crawl quotes (spyder name   quotes)

If some errors comes with missing library then install with conda.

22. In above code response consists of complete HTML element scraped from website.

23. In Anaconda powershell analyse the debug logs

     * Here 404: for file not found as this website dont have robots.txt file

     * 200: OK success status response code indicates that the request has succeeded. 

{'titletest': ['<title>Quotes to Scrape</title>']}  #here both tag and element of intrest is scraped.

To get only element of intrest add this line to your code (it has already added just uncoment)

title = response.css('title::text').extract()

save and run the spider output will be 

{'titletest': ['Quotes to Scrape']}   #Only element of intrest is scraped.

24. Scrapy framework requires follwing entities to be as it is in script: "  name, start_urls, parse " dont change these with any random names.

25. response consists of HTML file scraped from the traget website.

26. response.css are selectors which uses the logic of if condition (if this is the tag then splect the field of intrest)
      there are 2 type sof selectors CSS and HTML.

27. yeild: will return a ditionary operator.

28. So now we shall see  most popular or most commonly used scrapy practice. That is most of the time the scrapers will try to use command line property of the scrapt.

  Now go to power shell type: scrapy shell "website_of_your_intrest"
scrapy shell "https://quotes.toscrape.com/" hit enter

  Now you will be working in shell mode complete HTML code is with my

29. Now extract the file of intrest from HTML code base.

 #title = response.css('title').extract() #output=tage + string
  title = response.css('title::text').extract()  #output= string

           or

response.css('title').extract()
response.css('title::text').extract()


As the above said output is a list you can perform list operation on them

response.css('title::text')[0].extract()
output: quotes to scrape

response.css('title::text')[1].extract()
output: Error Message
   
30.

response.css('title::text').extract_first()
output: quotes to scrape

31. Now we shall extract quotes from the website so inspect the website and convey in which tag the quotes are.
In power shell: 

response.css('span.text::text').extract()  

#Note:in CSS class are represted by "." operator

output: All the quotes (as every quotes are present in span tag and also output is a list).

32. To display a specific Quote:

response.css('span.text::text')[0].extract()
response.css('span.text::text')[5].extract()















